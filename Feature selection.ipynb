{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many training points are there, according to the starter code?\n",
    "\n",
    "What’s the accuracy of the decision tree you just made?\n",
    "\n",
    "What’s the importance of the most important feature? What is the number of this feature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\victo\\Anaconda3\\envs\\py27\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\victo\\Anaconda3\\envs\\py27\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1039: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "0.9476678043230944\n",
      "the number of this feature is 33614, the importance is 0.764705882353.\n",
      "sshacklensf\n"
     ]
    }
   ],
   "source": [
    "# %load \"../mechine_learning_ex/feature_selection/find_signature.py\"\n",
    "#!/usr/bin/python\n",
    "\n",
    "import pickle\n",
    "import numpy\n",
    "numpy.random.seed(42)\n",
    "\n",
    "\n",
    "### The words (features) and authors (labels), already largely processed.\n",
    "### These files should have been created from the previous (Lesson 10)\n",
    "### mini-project.\n",
    "words_file = \"../mechine_learning_ex/text_learning/your_word_data.pkl\"\n",
    "authors_file = \"../mechine_learning_ex/text_learning/your_email_authors.pkl\"\n",
    "word_data = pickle.load( open(words_file, \"r\"))\n",
    "authors = pickle.load( open(authors_file, \"r\") )\n",
    "\n",
    "\n",
    "\n",
    "### test_size is the percentage of events assigned to the test set (the\n",
    "### remainder go into training)\n",
    "### feature matrices changed to dense representations for compatibility with\n",
    "### classifier functions in versions 0.15.2 and earlier\n",
    "from sklearn import cross_validation\n",
    "features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(word_data, authors, test_size=0.1, random_state=42)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "                             stop_words='english')\n",
    "features_train = vectorizer.fit_transform(features_train)\n",
    "features_test  = vectorizer.transform(features_test).toarray()\n",
    "\n",
    "\n",
    "### a classic way to overfit is to use a small number\n",
    "### of data points and a large number of features;\n",
    "### train on only 150 events to put ourselves in this regime\n",
    "features_train = features_train[:150].toarray()\n",
    "labels_train   = labels_train[:150]\n",
    "\n",
    "\n",
    "\n",
    "### your code goes here\n",
    "\n",
    "print(len(features_train))\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf= DecisionTreeClassifier()\n",
    "clf.fit(features_train,labels_train)\n",
    "print(clf.score(features_test,labels_test))\n",
    "\n",
    "\n",
    "\n",
    "### What’s the importance of the most important feature? What is the number of this feature?\n",
    "\n",
    "featurelist =[]\n",
    "importance = list(clf.feature_importances_)\n",
    "for num, i in enumerate(importance):\n",
    "    if i >0.2:\n",
    "        print(\"the number of this feature is {}, the importance is {}.\".format(num,i)) \n",
    "           \n",
    "### pull out the word that’s causing most of the discrimination of the decision tree. What is it?\n",
    "features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(word_data, authors, test_size=0.1, random_state=42)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "stop_words='english')\n",
    "features_train = vectorizer.fit_transform(features_train)\n",
    "print(vectorizer.get_feature_names()[33614])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove sshacklensf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\victo\\OneDrive\\桌面\\mechine_learning_ex\\text_learning\n"
     ]
    }
   ],
   "source": [
    "cd text_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('word_data[152] is:', u'tjonesnsf stephani and sam need nymex calendar')\n",
      "emails processed\n",
      "there are 38756 different word.\n",
      "('What is word number 34597 in your TfIdf?', u'stephanlonect')\n"
     ]
    }
   ],
   "source": [
    "# %load vectorize_text.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import sys\n",
    "\n",
    "sys.path.append( \"../tools/\" )\n",
    "from parse_out_email_text import parseOutText\n",
    "\"\"\"\n",
    "    Starter code to process the emails from Sara and Chris to extract\n",
    "    the features and get the documents ready for classification.\n",
    "\n",
    "    The list of all the emails from Sara are in the from_sara list\n",
    "    likewise for emails from Chris (from_chris)\n",
    "\n",
    "    The actual documents are in the Enron email dataset, which\n",
    "    you downloaded/unpacked in Part 0 of the first mini-project. If you have\n",
    "    not obtained the Enron email corpus, run startup.py in the tools folder.\n",
    "\n",
    "    The data is stored in lists and packed away in pickle files at the end.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from_sara  = open(\"from_sara.txt\", \"r\")\n",
    "from_chris = open(\"from_chris.txt\", \"r\")\n",
    "\n",
    "from_data = []\n",
    "word_data = []\n",
    "\n",
    "### temp_counter is a way to speed up the development--there are\n",
    "### thousands of emails from Sara and Chris, so running over all of them\n",
    "### can take a long time\n",
    "### temp_counter helps you only look at the first 200 emails in the list so you\n",
    "### can iterate your modifications quicker\n",
    "temp_counter = 0\n",
    "\n",
    "\n",
    "for name, from_person in [(\"sara\", from_sara), (\"chris\", from_chris)]:\n",
    "    for path in from_person:\n",
    "        ### only look at first 200 emails when developing\n",
    "        ### once everything is working, remove this line to run over full dataset\n",
    "        ##temp_counter += 1\n",
    "        if temp_counter < 200:\n",
    "            path = os.path.join('..', path[:-1])\n",
    "            ##print path\n",
    "            email = open(path, \"r\")\n",
    "\n",
    "            ### use parseOutText to extract the text from the opened email\n",
    "            text = parseOutText(email)\n",
    "            ### use str.replace() to remove any instances of the words\n",
    "            ### [\"sara\", \"shackleton\", \"chris\", \"germani\"]\n",
    "            remove =[\"sara\", \"shackleton\", \"chris\", \"germani\",\"sshacklensf\"]\n",
    "            for i in remove:\n",
    "                text = text.replace(i,'')\n",
    "            ### append the text to word_data\n",
    "            word_data.append(text)\n",
    "            ### append a 0 to from_data if email is from Sara, and 1 if email is from Chris\n",
    "            from_data.append(0 if name == 'sara' else 1)\n",
    "\n",
    "            email.close()\n",
    "print(\"word_data[152] is:\",word_data[152])\n",
    "\n",
    "print \"emails processed\"\n",
    "from_sara.close()\n",
    "from_chris.close()\n",
    "\n",
    "pickle.dump( word_data, open(\"your_word_data.pkl\", \"w\") )\n",
    "pickle.dump( from_data, open(\"your_email_authors.pkl\", \"w\") )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### in Part 4, do TfIdf vectorization here\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words= 'english')\n",
    "x = vectorizer.fit_transform(word_data)\n",
    "print(\"there are {} different word.\".format(x.shape[1]))\n",
    "print(\"What is word number 34597 in your TfIdf?\",vectorizer.get_feature_names()[34597])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'C:\\\\Users\\\\victo\\\\OneDrive\\\\\\u684c\\u9762\\\\mechine_learning_ex\\\\text_learning'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\victo\\OneDrive\\桌面\\mechine_learning_ex\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any other outliers pop up? What word is it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "0.9505119453924915\n",
      "the number of this feature is 14343, the importance is 0.666666666667.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "u'cgermannsf'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %load ../mechine_learning_ex/feature_selection/find_signature.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import pickle\n",
    "import numpy\n",
    "numpy.random.seed(42)\n",
    "\n",
    "\n",
    "### The words (features) and authors (labels), already largely processed.\n",
    "### These files should have been created from the previous (Lesson 10)\n",
    "### mini-project.\n",
    "words_file = \"../mechine_learning_ex/text_learning/your_word_data.pkl\"\n",
    "authors_file = \"../mechine_learning_ex/text_learning/your_email_authors.pkl\"\n",
    "word_data = pickle.load( open(words_file, \"r\"))\n",
    "authors = pickle.load( open(authors_file, \"r\") )\n",
    "\n",
    "\n",
    "\n",
    "### test_size is the percentage of events assigned to the test set (the\n",
    "### remainder go into training)\n",
    "### feature matrices changed to dense representations for compatibility with\n",
    "### classifier functions in versions 0.15.2 and earlier\n",
    "from sklearn import cross_validation\n",
    "features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(word_data, authors, test_size=0.1, random_state=42)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "                             stop_words='english')\n",
    "features_train = vectorizer.fit_transform(features_train)\n",
    "features_test  = vectorizer.transform(features_test).toarray()\n",
    "\n",
    "\n",
    "### a classic way to overfit is to use a small number\n",
    "### of data points and a large number of features;\n",
    "### train on only 150 events to put ourselves in this regime\n",
    "features_train = features_train[:150].toarray()\n",
    "labels_train   = labels_train[:150]\n",
    "\n",
    "\n",
    "\n",
    "### your code goes here\n",
    "\n",
    "print(len(features_train))\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf= DecisionTreeClassifier()\n",
    "clf.fit(features_train,labels_train)\n",
    "print(clf.score(features_test,labels_test))\n",
    "\n",
    "\n",
    "\n",
    "### What’s the importance of the most important feature? What is the number of this feature?\n",
    "\n",
    "featurelist =[]\n",
    "importance = list(clf.feature_importances_)\n",
    "for num, i in enumerate(importance):\n",
    "    if i >0.2:\n",
    "        print(\"the number of this feature is {}, the importance is {}.\".format(num,i))\n",
    "\n",
    "### Any other outliers pop up? What word is it?\n",
    "from sklearn import cross_validation\n",
    "features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(word_data, authors, test_size=0.1, random_state=42)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "                             stop_words='english')\n",
    "features_train = vectorizer.fit_transform(features_train)\n",
    "features_test  = vectorizer.transform(features_test).toarray()\n",
    "vectorizer.get_feature_names()[14343]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update vectorize_test.py one more time, and rerun. Then run find_signature.py again. Any other important features (importance>0.2) arise? How many? Do any of them look like “signature words”, or are they more “email content” words, that look like they legitimately come from the text of the messages?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\victo\\OneDrive\\桌面\\mechine_learning_ex\\text_learning\n"
     ]
    }
   ],
   "source": [
    "cd text_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('word_data[152] is:', u'tjonesnsf stephani and sam need nymex calendar')\n",
      "emails processed\n",
      "there are 38755 different word.\n",
      "('What is word number 34597 in your TfIdf?', u'stephen')\n"
     ]
    }
   ],
   "source": [
    "# %load vectorize_text.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import sys\n",
    "\n",
    "sys.path.append( \"../tools/\" )\n",
    "from parse_out_email_text import parseOutText\n",
    "\"\"\"\n",
    "    Starter code to process the emails from Sara and Chris to extract\n",
    "    the features and get the documents ready for classification.\n",
    "\n",
    "    The list of all the emails from Sara are in the from_sara list\n",
    "    likewise for emails from Chris (from_chris)\n",
    "\n",
    "    The actual documents are in the Enron email dataset, which\n",
    "    you downloaded/unpacked in Part 0 of the first mini-project. If you have\n",
    "    not obtained the Enron email corpus, run startup.py in the tools folder.\n",
    "\n",
    "    The data is stored in lists and packed away in pickle files at the end.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from_sara  = open(\"from_sara.txt\", \"r\")\n",
    "from_chris = open(\"from_chris.txt\", \"r\")\n",
    "\n",
    "from_data = []\n",
    "word_data = []\n",
    "\n",
    "### temp_counter is a way to speed up the development--there are\n",
    "### thousands of emails from Sara and Chris, so running over all of them\n",
    "### can take a long time\n",
    "### temp_counter helps you only look at the first 200 emails in the list so you\n",
    "### can iterate your modifications quicker\n",
    "temp_counter = 0\n",
    "\n",
    "\n",
    "for name, from_person in [(\"sara\", from_sara), (\"chris\", from_chris)]:\n",
    "    for path in from_person:\n",
    "        ### only look at first 200 emails when developing\n",
    "        ### once everything is working, remove this line to run over full dataset\n",
    "        ##temp_counter += 1\n",
    "        if temp_counter < 200:\n",
    "            path = os.path.join('..', path[:-1])\n",
    "            ##print path\n",
    "            email = open(path, \"r\")\n",
    "\n",
    "            ### use parseOutText to extract the text from the opened email\n",
    "            text = parseOutText(email)\n",
    "            ### use str.replace() to remove any instances of the words\n",
    "            ### [\"sara\", \"shackleton\", \"chris\", \"germani\"]\n",
    "            remove =[\"sara\", \"shackleton\", \"chris\", \"germani\",\"sshacklensf\",\"cgermannsf\"]\n",
    "            for i in remove:\n",
    "                text = text.replace(i,'')\n",
    "            ### append the text to word_data\n",
    "            word_data.append(text)\n",
    "            ### append a 0 to from_data if email is from Sara, and 1 if email is from Chris\n",
    "            from_data.append(0 if name == 'sara' else 1)\n",
    "\n",
    "            email.close()\n",
    "print(\"word_data[152] is:\",word_data[152])\n",
    "\n",
    "print \"emails processed\"\n",
    "from_sara.close()\n",
    "from_chris.close()\n",
    "\n",
    "pickle.dump( word_data, open(\"your_word_data.pkl\", \"w\") )\n",
    "pickle.dump( from_data, open(\"your_email_authors.pkl\", \"w\") )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### in Part 4, do TfIdf vectorization here\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words= 'english')\n",
    "x = vectorizer.fit_transform(word_data)\n",
    "print(\"there are {} different word.\".format(x.shape[1]))\n",
    "print(\"What is word number 34597 in your TfIdf?\",vectorizer.get_feature_names()[34597])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\victo\\OneDrive\\桌面\\mechine_learning_ex\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "0.8168373151308305\n",
      "the number of this feature is 21323, the importance is 0.363636363636.\n",
      "houectect\n"
     ]
    }
   ],
   "source": [
    "# %load ../mechine_learning_ex/feature_selection/find_signature.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import pickle\n",
    "import numpy\n",
    "numpy.random.seed(42)\n",
    "\n",
    "\n",
    "### The words (features) and authors (labels), already largely processed.\n",
    "### These files should have been created from the previous (Lesson 10)\n",
    "### mini-project.\n",
    "words_file = \"../mechine_learning_ex/text_learning/your_word_data.pkl\"\n",
    "authors_file = \"../mechine_learning_ex/text_learning/your_email_authors.pkl\"\n",
    "word_data = pickle.load( open(words_file, \"r\"))\n",
    "authors = pickle.load( open(authors_file, \"r\") )\n",
    "\n",
    "\n",
    "\n",
    "### test_size is the percentage of events assigned to the test set (the\n",
    "### remainder go into training)\n",
    "### feature matrices changed to dense representations for compatibility with\n",
    "### classifier functions in versions 0.15.2 and earlier\n",
    "from sklearn import cross_validation\n",
    "features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(word_data, authors, test_size=0.1, random_state=42)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "                             stop_words='english')\n",
    "features_train = vectorizer.fit_transform(features_train)\n",
    "features_test  = vectorizer.transform(features_test).toarray()\n",
    "\n",
    "\n",
    "### a classic way to overfit is to use a small number\n",
    "### of data points and a large number of features;\n",
    "### train on only 150 events to put ourselves in this regime\n",
    "features_train = features_train[:150].toarray()\n",
    "labels_train   = labels_train[:150]\n",
    "\n",
    "\n",
    "\n",
    "### your code goes here\n",
    "\n",
    "print(len(features_train))\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf= DecisionTreeClassifier()\n",
    "clf.fit(features_train,labels_train)\n",
    "print(clf.score(features_test,labels_test))\n",
    "\n",
    "\n",
    "\n",
    "### What’s the importance of the most important feature? What is the number of this feature?\n",
    "\n",
    "featurelist =[]\n",
    "importance = list(clf.feature_importances_)\n",
    "for num, i in enumerate(importance):\n",
    "    if i >0.2:\n",
    "        print(\"the number of this feature is {}, the importance is {}.\".format(num,i))\n",
    "\n",
    "### pull out the word that’s causing most of the discrimination of the decision tree. What is it?\n",
    "features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(word_data, authors, test_size=0.1, random_state=42)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "stop_words='english')\n",
    "features_train = vectorizer.fit_transform(features_train)\n",
    "print(vectorizer.get_feature_names()[21323])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
